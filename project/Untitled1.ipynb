{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qYO2MBEk0n00"
   },
   "outputs": [],
   "source": [
    "from model import *\n",
    "from data.data_loader import *\n",
    "from config import *\n",
    "from scipy.sparse import csr_matrix\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "projectPath='models/dummy/'\n",
    "useSavedModel=False\n",
    "\n",
    "print(\"Loading the dataset\")\n",
    "Source_x, Source_y, Target_x = prepare_Data(data_dir, True)\n",
    "# Gallery_x, Query_x = prepare_Data(data_dir, False)\n",
    "label_Similarity = csr_matrix(scipy.io.loadmat(\"data/cifar10/cifar10_Similarity.mat\")['label_Similarity']).todense()\n",
    "print(\"Data loading finished\")\n",
    "\n",
    "source = torch.utils.data.DataLoader([(Source_x[i], Source_y[i]) for i in range(len(Source_x))],batch_size=batchSize)\n",
    "target = torch.utils.data.DataLoader(Target_x,batch_size=batchSize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading the models\")\n",
    "device = torch.device(\"cuda:0\")\n",
    "Net = GPQModel().to(device)\n",
    "Prototypes = IntraNorm(Net.C, numCodeBooks).to(device)\n",
    "Z = softAssignment(Prototypes,Net.Z,torch.tensor(numCodeBooks).to(device),torch.tensor(softAssgnAlpha).to(device))\n",
    "optimizer = optim.Adam(Net.parameters(),lr=0.0002,betas=(0.5,0.999))\n",
    "\n",
    "numLabelledSamples=Source_x.shape[0]\n",
    "numUnlabelledSamples=Target_x.shape[0]\n",
    "numIterations=int(numLabelledSamples/batchSize)\n",
    "startEpoch=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numLabelledSamples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_ = iter(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "20*300 / 3600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(1):\n",
    "    for batch in tqdm(source):\n",
    "        XLabelled, YLabelled = batch\n",
    "        YLabelled = YLabelled.to(device)\n",
    "        XLabelled = torch.tensor(data_augmentation(XLabelled)).to(device)\n",
    "        try: \n",
    "            XUnlabelled = next(target_)\n",
    "            XUnlabelled = torch.tensor(data_augmentation(XUnlabelled)).to(device)\n",
    "        except:\n",
    "            target_ = iter(target)\n",
    "            XUnlabelled = next(target_)\n",
    "            XUnlabelled = torch.tensor(data_augmentation(XUnlabelled)).to(device)\n",
    "        \n",
    "        YLabelled=torch.eye(numClasses)[YLabelled]\n",
    "        YLabelledMat=torch.matmul(YLabelled,YLabelled.T)\n",
    "        YLabelledMat/=torch.sum(YLabelledMat,axis=1,keepdims=True)\n",
    "        YLabelledMat = YLabelledMat.to(device)\n",
    "    \n",
    "        feature_S=Net.featureExtractor(XLabelled.reshape(batchSize,3,32,32))\n",
    "        feature_T=flipGradient(Net.featureExtractor(XUnlabelled.reshape(batchSize,3,32,32).clone().detach()))\n",
    "        \n",
    "        feature_S=IntraNorm(feature_S,numCodeBooks)\n",
    "        feature_T=IntraNorm(feature_T,numCodeBooks)\n",
    "        \n",
    "        descriptor_S=softAssignment(Z,feature_S,numCodeBooks,softAssgnAlpha)\n",
    "        logits_S=Net.classifier(feature_S*beta,Prototypes*beta)\n",
    "        hash_loss = NPQLoss(YLabelledMat,feature_S, descriptor_S,numCodeBooks)\n",
    "        \n",
    "        break\n",
    "        cls_loss = CLSLoss(YLabelled,logits_S)\n",
    "        entropy_loss = SMELoss(feature_T * beta, Prototypes * beta, numCodeBooks)\n",
    "        final_loss = hash_loss + lam_1*entropy_loss + lam_2*cls_loss \n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        final_loss.backward(retain_graph=True)\n",
    "        optimizer.step()\n",
    "        \n",
    "        epochLoss+=final_loss.item()\n",
    "        if iterationIndex==numIterations-1:\n",
    "            print(\"Final loss : \",final_loss,\" of epoch : \",epoch)\n",
    "    \n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_loss = CLSLoss(YLabelled,logits_S)\n",
    "# entropy_loss = SMELoss(feature_T * beta, Prototypes * beta, numCodeBooks)\n",
    "# final_loss = hash_loss + lam_1*entropy_loss + lam_2*cls_loss \n",
    "\n",
    "# optimizer.zero_grad()\n",
    "# final_loss.backward(retain_graph=True)\n",
    "# optimizer.step()\n",
    "\n",
    "# epochLoss+=final_loss.item()\n",
    "# if iterationIndex==numIterations-1:\n",
    "#     print(\"Final loss : \",final_loss,\" of epoch : \",epoch)\n",
    "\n",
    "# #         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits_S.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "YLabelledMat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(startEpoch,totalEpochs,1):\n",
    "    epochLoss=0\n",
    "    \n",
    "    for iterationIndex in range(numIterations):\n",
    "        \n",
    "        labelledIndices=np.random.choice(numLabelledSamples,size=batchSize,replace=False)\n",
    "        unlabelledIndices=np.random.choice(numUnlabelledSamples,size=batchSize,replace=False)\n",
    "        \n",
    "        XLabelled=Source_x[labelledIndices]\n",
    "        YLabelled=Source_y[labelledIndices]\n",
    "        XUnlabelled=Target_x[unlabelledIndices]\n",
    "        XLabelled=np.asarray(data_augmentation(XLabelled))\n",
    "        XLabelled=torch.from_numpy(XLabelled)\n",
    "        XUnlabelled=np.asarray(data_augmentation(XUnlabelled))\n",
    "        XUnlabelled=torch.from_numpy(XUnlabelled)\n",
    "        \n",
    "        YLabelled=np.eye(numClasses)[YLabelled]\n",
    "        YLabelledMat=np.matmul(YLabelled,YLabelled.transpose())\n",
    "        YLabelledMat/=np.sum(YLabelledMat,axis=1,keepdims=True)\n",
    "        \n",
    "        feature_S=Net.featureExtractor(XLabelled.reshape(batchSize,3,32,32))\n",
    "        feature_T=flipGradient(Net.featureExtractor(XUnlabelled.reshape(batchSize,3,32,32).clone().detach()))\n",
    "        \n",
    "        feature_S=IntraNorm(feature_S,numCodeBooks)\n",
    "        feature_T=IntraNorm(feature_T,numCodeBooks)\n",
    "        \n",
    "        descriptor_S=softAssignment(Z,feature_S,numCodeBooks,softAssgnAlpha)\n",
    "        logits_S=Net.classifier(feature_S*beta,Prototypes*beta)\n",
    "        hash_loss = NPQLoss(torch.from_numpy(YLabelledMat),feature_S, descriptor_S,numCodeBooks)\n",
    "        \n",
    "        cls_loss = CLSLoss(torch.from_numpy(YLabelled),logits_S)\n",
    "        entropy_loss = SMELoss(feature_T * beta, Prototypes * beta, numCodeBooks)\n",
    "        final_loss = hash_loss + lam_1*entropy_loss + lam_2*cls_loss \n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        final_loss.backward(retain_graph=True)\n",
    "        optimizer.step()\n",
    "        \n",
    "        epochLoss+=final_loss.item()\n",
    "        if iterationIndex==numIterations-1:\n",
    "            print(\"Final loss : \",final_loss,\" of epoch : \",epoch)\n",
    "    \n",
    "    epochLoss=epochLoss/numIterations\n",
    "    allEpochLoss.append(epochLoss)\n",
    "    \n",
    "    stateToBeSaved={\n",
    "      'startEpoch': epoch,\n",
    "      'modelStateDict': Net.state_dict(),\n",
    "      'optimizer' : optimizer.state_dict(),\n",
    "      'Z':Z,\n",
    "      'Prototypes':Prototypes,\n",
    "      'allEpochLoss':allEpochLoss\n",
    "    }\n",
    "    \n",
    "    if epoch%10==0:\n",
    "        checkPointFile=projectPath+\"saved/GPQModel\"+str(epoch)+\".pth.tar\"\n",
    "        torch.save(stateToBeSaved,checkPointFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jmfJV-TZ0n00",
    "outputId": "f85cea51-5574-4350-9d74-89f0dc282cb6"
   },
   "outputs": [],
   "source": [
    "\n",
    "print(\"Loading the dataset\")\n",
    "Source_x, Source_y, Target_x = prepare_Data(data_dir, True)\n",
    "Gallery_x, Query_x = prepare_Data(data_dir, False)\n",
    "print(\"Data loading finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FSfSoF6w0n01",
    "outputId": "6a8c285a-c359-44b6-a948-491d9da8fd34"
   },
   "outputs": [],
   "source": [
    "print(\"Loading the models\")\n",
    "Net = GPQModel()\n",
    "Prototypes = IntraNorm(Net.C, numCodeBooks)\n",
    "Z = softAssignment(Prototypes, Net.Z, numCodeBooks, softAssgnAlpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t2EjIZOJ0n01"
   },
   "outputs": [],
   "source": [
    "a = torch.utils.data.DataLoader(Source_x, batch_size = 1)\n",
    "b = torch.utils.data.DataLoader(Source_y, batch_size = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "axG7ZE8k0n01",
    "outputId": "cd6f821b-f372-400f-a6da-b276aeb1dcd1"
   },
   "outputs": [],
   "source": [
    "numCodeBooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gN8vyYpJ0n01",
    "outputId": "1dd23b88-ef76-490e-eef4-23eb29db9204"
   },
   "outputs": [],
   "source": [
    "beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AzHqLUMJ0n01",
    "outputId": "2316b698-3922-4068-f495-9fd15e4b41cc"
   },
   "outputs": [],
   "source": [
    "numClasses, numCodeBooks, lenCodeWord, 144"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5lonkNeP0n01",
    "outputId": "16936db0-bb59-4256-9c3f-8446f4da774e"
   },
   "outputs": [],
   "source": [
    "(10, 144)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "73RrC99u0n01"
   },
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KvNIjMLE0n01"
   },
   "outputs": [],
   "source": [
    "label_Similarity = csr_matrix(scipy.io.loadmat(\"data/cifar10/cifar10_Similarity.mat\")['label_Similarity'])\n",
    "label_Similarity = label_Similarity.todense()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0wrPF55t0n01",
    "outputId": "723b1145-60f1-4e6d-8d55-8047ca8c321b"
   },
   "outputs": [],
   "source": [
    "label_Similarity.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WZq8jJaU0n01",
    "outputId": "c9a215b5-0e98-48a6-91d8-dd47bea8adea"
   },
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(Net.parameters(), lr=0.01, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f06DjtC40n01",
    "outputId": "de135e9c-ebad-40bf-e9fe-1e77288fe45e",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "for i in a:\n",
    "    feature_S = Net.featureExtractor(i.reshape(1,3,32,32))\n",
    "    feature_T = flipGradient(Net.featureExtractor(i.reshape(1,3,32,32).clone().detach()))\n",
    "    \n",
    "    feature_S = IntraNorm(feature_S, numCodeBooks)\n",
    "    feature_T = IntraNorm(feature_T, numCodeBooks)\n",
    "    \n",
    "    descriptor_S = softAssignment(Z, feature_S, numCodeBooks, softAssgnAlpha)\n",
    "    logits_S = Net.classifier(feature_S, Prototypes)\n",
    "    hash_loss = NPQLoss(label_Similarity, feature_S, descriptor_S,numCodeBooks)\n",
    "    #print(\"Hash loss : \",hash_loss)\n",
    "    \n",
    "    for k in b:\n",
    "        cls_loss = CLSLoss(k, logits_S)\n",
    "        break\n",
    "    #print(\"Cls loss : \",cls_loss)\n",
    "    entropy_loss = SMELoss(feature_S, Prototypes, numCodeBooks)    \n",
    "    \n",
    "    final_loss = hash_loss + cls_loss + entropy_loss\n",
    "    print(\"Final loss : \",final_loss)\n",
    "    final_loss.backward()\n",
    "    optimizer.step()\n",
    "    print(\"Done optimization\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "04ilIg6h0n01",
    "outputId": "c59b30de-a49c-46f8-fa87-3fa17774634a"
   },
   "outputs": [],
   "source": [
    "Prototypes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y_XOVbaw0n02"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Untitled1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
