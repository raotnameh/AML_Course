{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import *\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "from data.data_loader import *\n",
    "from model import *\n",
    "import torch.optim as optim\n",
    "from  RetrievalTest import *\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "batchSize = 1000\n",
    "\n",
    "print(\"Loading the dataset\")\n",
    "Source_x, Source_y, Target_x = prepare_Data(data_dir, True)\n",
    "Gallery_x, Query_x = prepare_Data(data_dir, False)\n",
    "similarity = csr_matrix(scipy.io.loadmat(\"data/cifar10/cifar10_Similarity.mat\")['label_Similarity']).todense()\n",
    "\n",
    "print(\"Data loading finished\")\n",
    "\n",
    "gallery = torch.utils.data.DataLoader(Gallery_x,batch_size=1000)\n",
    "query = torch.utils.data.DataLoader(Query_x,batch_size=1000)\n",
    "\n",
    "source = torch.utils.data.DataLoader([(Source_x[i], Source_y[i]) for i in range(len(Source_x))],batch_size=batchSize, shuffle=True)\n",
    "target = torch.utils.data.DataLoader(Target_x,batch_size=batchSize,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vggModel=models.vgg16_bn(pretrained=True)\n",
    "# vggModel.children\n",
    "\n",
    "net1, net2 = [], []\n",
    "for i in vggModel.children():\n",
    "    for r, i in enumerate(i.children()):\n",
    "        if r <=23: net1.append(i)\n",
    "        elif r<= 32: net2.append(i)\n",
    "    break\n",
    "net1, net2  = nn.Sequential(*net1), nn.Sequential(*net2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def intranorm(features,n_book):\n",
    "    x = features.split(n_book,1)\n",
    "    \n",
    "    for b in range(n_book):\n",
    "        if b==0: dummy = F.normalize(x[b],1)\n",
    "        else:\n",
    "            dummy = torch.cat((dummy,F.normalize(x[b],1)),1)\n",
    "    return dummy\n",
    "\n",
    "def shape_(inp):\n",
    "     for i in inp:\n",
    "            print(f\"shape is: {i.shape}\")\n",
    "            \n",
    "def Indexing_(Z,des,numSeg):\n",
    "        Z = intranorm(Z,numSeg)\n",
    "        x = torch.split(des,numSeg,1)\n",
    "        y = torch.split(Z,numSeg,1)\n",
    "        for i in range(numSeg):\n",
    "            size_x = x[i].shape[0]\n",
    "            size_y = y[i].shape[0]\n",
    "            xx = x[i].unsqueeze(-1)\n",
    "\n",
    "            dummy = torch.tensor(1)\n",
    "            xx = xx.tile([1,1,size_y])\n",
    "            yy = y[i].unsqueeze(-1)\n",
    "            yy = yy.tile([1,1,size_x])\n",
    "            yy = yy.permute(2,1,0)\n",
    "            diff = torch.sum(torch.multiply(xx,yy),1)\n",
    "\n",
    "            arg = torch.argmax(diff,1)\n",
    "            max_idx = arg.reshape(-1,1)\n",
    "\n",
    "            if i == 0: quant_idx = max_idx\n",
    "            else: quant_idx = torch.cat((quant_idx,max_idx),1)\n",
    "\n",
    "        return quant_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class softassignment_(nn.Module):\n",
    "    def __init__(self,len_code, n_book, intn_word):\n",
    "        super(softassignment_,self).__init__()\n",
    "        self.Z = nn.Linear(len_code * n_book,intn_word, bias=False)\n",
    "\n",
    "    def forward(self,features,n_book,alpha,):\n",
    "        z_ = intranorm(self.Z.state_dict()['weight'], n_book).split(n_book,1)\n",
    "        x_ = features.split(n_book,1)\n",
    "\n",
    "        for i in range(n_book):\n",
    "            size_z = z_[i].shape[0] # number of codewords\n",
    "            size_x = x_[i].shape[0] # batch size\n",
    "            xx = x_[i].unsqueeze(-1)\n",
    "            xx = xx.repeat(1,1,size_z)\n",
    "            zz = z_[i].unsqueeze(-1)\n",
    "            zz = zz.repeat(1,1,size_x).T\n",
    "\n",
    "            diff = 1 - torch.sum(torch.mul(xx,zz), 1) # 32,16\n",
    "            softmax_diff = F.softmax(diff*(-alpha),1) #32,16\n",
    "            soft_des_temp = torch.matmul(softmax_diff,z_[i]) # 32,12\n",
    "            if i == 0: descriptor = soft_des_temp\n",
    "            else: descriptor = torch.cat((descriptor,soft_des_temp),1)\n",
    "\n",
    "        return intranorm(descriptor,n_book) # 32,144"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class classifier_(nn.Module):\n",
    "    def __init__(self,n_CLASSES, len_code, n_book,):\n",
    "        super(classifier_,self).__init__()\n",
    "        self.prototypes = nn.Linear(len_code * n_book, n_CLASSES, bias=False)\n",
    "        self.n_book = n_book\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x_ = x.split(self.n_book,1)\n",
    "        c_ = (intranorm(self.prototypes.state_dict()['weight'], n_book)*beta).T.split(self.n_book,0)\n",
    "        for i in range(self.n_book):\n",
    "            sub_res = torch.matmul(x_[i], c_[i]).unsqueeze(-1)\n",
    "            if i == 0: res = sub_res\n",
    "            else: res = torch.cat((res,sub_res),2)\n",
    "        \n",
    "        return torch.sum(res, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class features_(nn.Module):\n",
    "    def __init__(self, net1, net2):\n",
    "        super(features_,self).__init__()\n",
    "        \n",
    "        self.net1 = net1\n",
    "        self.net2 = net2\n",
    "        self.gavgp = nn.AdaptiveAvgPool2d(1)\n",
    "        self.linear = nn.Linear(768, len_code*n_book)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = self.net1(x) # shape: torch.Size([32, 3, 32,32])>torch.Size([32, 3, 4, 4])\n",
    "        x_branch = self.gavgp(x)\n",
    "        x = self.net2(x) # shape: torch.Size([32, 3, 4, 4])> torch.Size([32, 3, 4, 4])\n",
    "        x = self.gavgp(x)\n",
    "        \n",
    "        x = torch.cat((x,x_branch),1)\n",
    "        \n",
    "        return self.linear(x.view(-1,768))\n",
    "    \n",
    "# x = torch.randn(32, 3, 32, 32, device='cpu')\n",
    "# model = features_(net1, net2)\n",
    "# out = model(x)# model\n",
    "# out.shape, out.split(12,1)[0].shape, model.Z.shape, model.prototypes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class flipGradient_(nn.Module):\n",
    "    def forward(self,x,l=1.0):\n",
    "        positivePath=(x*2).clone().detach().requires_grad_(False)\n",
    "        negativePath=(-x).requires_grad_(True)\n",
    "        return positivePath+negativePath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#models\n",
    "model = features_(net1, net2).to(device)\n",
    "forget_net = features_(net1, net2).to(device)\n",
    "classifier = classifier_(n_CLASSES, len_code, n_book).to(device)\n",
    "discriminator = classifier_(2, len_code, n_book).to(device)\n",
    "softassignment = softassignment_(len_code, n_book, intn_word).to(device)\n",
    "# flipGradient = flipGradient_()\n",
    "\n",
    "# optimizer\n",
    "class_optim = optim.Adam(classifier.parameters(),lr=0.002,weight_decay=0.00001,amsgrad=True)\n",
    "model_optim = optim.Adam(model.parameters(),lr=0.0002,weight_decay=0.00001,amsgrad=True)\n",
    "soft_optim = optim.Adam(softassignment.parameters(),lr=0.002,weight_decay=0.00001,amsgrad=True)\n",
    "forg_optim = optim.Adam(forget_net.parameters(),lr=0.002,weight_decay=0.00001,amsgrad=True)\n",
    "disc_optim = optim.Adam(discriminator.parameters(),lr=0.002,weight_decay=0.00001,amsgrad=True)\n",
    "\n",
    "class_optim.zero_grad()\n",
    "model_optim.zero_grad()\n",
    "soft_optim.zero_grad()\n",
    "forg_optim.zero_grad()\n",
    "disc_optim.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dummy_target = torch.cat((torch.tensor(1).tile((batchSize,)), torch.tensor(0).tile((batchSize,))),0).to(device)\n",
    "target_ = iter(target)\n",
    "score = 0\n",
    "save = \"\"\n",
    "scoore = ''\n",
    "for epoch in tqdm(range(50)):\n",
    "    m_,n,o,p = 0,0,0,0\n",
    "    model.train()\n",
    "    classifier.train()\n",
    "    softassignment.train()\n",
    "    discriminator.train()\n",
    "    forget_net.train()\n",
    "    for df, batch in enumerate(source):\n",
    "        x, y = batch\n",
    "        if x.shape[0] < batchSize: break\n",
    "        x = torch.tensor(data_augmentation(x)).to(device)\n",
    "        y = y.to(device)\n",
    "        try: \n",
    "            xu = next(target_)\n",
    "            xu = torch.tensor(data_augmentation(xu)).to(device)\n",
    "        except:\n",
    "            target_ = iter(target)\n",
    "            xu = next(target_)\n",
    "            xu = torch.tensor(data_augmentation(xu)).to(device)\n",
    "\n",
    "        # adversarial forgetting\n",
    "        for g in range(1):\n",
    "            input_ = torch.cat((x,xu),0)\n",
    "            z = intranorm(model(input_.permute(0,3,1,2)), n_book)\n",
    "            m = intranorm(forget_net(input_.permute(0,3,1,2)), n_book)\n",
    "            z_ = z * m\n",
    "            \n",
    "            pred = discriminator(z_)\n",
    "            dummy_loss = torch.nn.functional.cross_entropy(pred,dummy_target)\n",
    "        \n",
    "            disc_optim.zero_grad()\n",
    "            forg_optim.zero_grad()\n",
    "            dummy_loss.backward()\n",
    "            disc_optim.step()\n",
    "\n",
    "        class_optim.zero_grad()\n",
    "        model_optim.zero_grad()\n",
    "        soft_optim.zero_grad()\n",
    "        forg_optim.zero_grad()\n",
    "        disc_optim.zero_grad()\n",
    "        \n",
    "        input_ = torch.cat((x,xu),0)\n",
    "        features = intranorm(model(x.permute(0,3,1,2)), n_book)\n",
    "        features_ = intranorm(model(xu.permute(0,3,1,2)), n_book)\n",
    "        z = torch.cat((features,features_),0)\n",
    "        m = intranorm(forget_net(input_.permute(0,3,1,2)), n_book)\n",
    "        z_ = z * m\n",
    "\n",
    "        pred = discriminator(z_)\n",
    "        quanta = softassignment(features,n_book,alpha)\n",
    "        logits = classifier(features *beta)\n",
    "        \n",
    "        entropy_loss = torch.nn.functional.cross_entropy(pred,torch.flip(dummy_target,(0,))) *0.1\n",
    "        mask_regulariser_loss = (m * (1-m)).mean() \n",
    "        \n",
    "        entropy_loss.backward(retain_graph=True)\n",
    "        model_optim.zero_grad()\n",
    "        \n",
    "        cls_loss = torch.nn.functional.cross_entropy(logits,y)\n",
    "\n",
    "        y = torch.eye(numClasses)[y].to(device)\n",
    "        y_ = torch.matmul(y,y.T)\n",
    "        y_ = y_/torch.sum(y_, axis=1, keepdims=True)\n",
    "        hash_loss = NPQLoss(y_,features, quanta,n_book)  \n",
    "        \n",
    "        final_loss = hash_loss + cls_loss*0.1  + mask_regulariser_loss*0.1\n",
    "        final_loss.backward()\n",
    "        \n",
    "        o += cls_loss.item()\n",
    "        m_ += final_loss.item()\n",
    "        n += hash_loss.item()\n",
    "        p += entropy_loss.item()\n",
    "\n",
    "        model_optim.step()\n",
    "        soft_optim.step()\n",
    "        class_optim.step()\n",
    "        forg_optim.step()\n",
    "\n",
    "    print(f\"epoch:{epoch+1}\\t{m_}\\t{n}\\t{o}\\t{p}\\n\")\n",
    "    save += f\"{m_/10}\\t{n/10}\\t{o/10}\\t{p/10}\\n\"\n",
    "    with open(\"analysis.txt\", \"w\") as f:\n",
    "        f.write(save)\n",
    "#     continue\n",
    "    \n",
    "    if epoch % 5 == 0:\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            forget_net.eval()\n",
    "            \n",
    "            for r, i in tqdm(enumerate(gallery)):\n",
    "                dfg = intranorm(model(i.to(device).permute(0,3,1,2)),n_book) * intranorm(forget_net(i.to(device).permute(0,3,1,2)), n_book)\n",
    "                if r == 0: \n",
    "                    dummy = dfg\n",
    "                else: \n",
    "                    dummy = torch.cat((dummy, dfg), 0)\n",
    "\n",
    "            for r, i in tqdm(enumerate(query)):\n",
    "                dfg = intranorm(model(i.to(device).permute(0,3,1,2)),n_book) * intranorm(forget_net(i.to(device).permute(0,3,1,2)), n_book)\n",
    "                if r == 0: \n",
    "                    query_x = dfg\n",
    "                else: \n",
    "                    query_x = torch.cat((query_x, dfg), 0)\n",
    "\n",
    "\n",
    "        dummy = Indexing_(softassignment.Z.state_dict()['weight'].cpu(), dummy.cpu(), n_book)\n",
    "        gallery_x = dummy.numpy().astype(int)\n",
    "        quantizedDist = pqDist(intranorm(softassignment.Z.state_dict()['weight'].cpu(), n_book), n_book,gallery_x, query_x.cpu().numpy()).T\n",
    "        Rank = np.argsort(quantizedDist, axis=0)\n",
    "        mean_average_precision=cat_apcal(similarity,Rank,54000)\n",
    "        scoore += f\"{mean_average_precision}\\n\"\n",
    "        if mean_average_precision > score:\n",
    "            score = mean_average_precision\n",
    "            stateToBeSaved={\n",
    "                'modelStateDict': [model.state_dict(),classifier.state_dict(), softassignment.state_dict(), forget_net.state_dict()],\n",
    "                'score': mean_average_precision,}\n",
    "            torch.save(stateToBeSaved,f\"{epoch}_mod2_hemant.pth\")\n",
    "        print(mean_average_precision)\n",
    "    with open(\"score.txt\", \"w\") as f:\n",
    "        f.write(save)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# THE END"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
