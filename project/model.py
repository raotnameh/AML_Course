# -*- coding: utf-8 -*-
"""AML-Project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1wEub0Lz0qr04Jmsr9n913fcSZ0bqV17Z
"""

import torch
import torch.nn as nn
import torchvision
from torchvision import datasets, models, transforms
from torch.autograd import Variable
import torch.nn.functional as F

#Setting the variables
numClasses=10
numCodeWords=16
numCodeBooks=12
lenCodeWord=12
softAssgnAlpha=20.0  # Soft assignment Scaling Factor
beta = 4

vggModel=models.vgg16_bn(pretrained=True)
for child in vggModel.children():
   childCounter=0
   if child.children():
      requiredLayers1=nn.Sequential(*list(child.children())[:23])
      requiredLayers2=nn.Sequential(*list(child.children())[24:33])
   break
  
class GPQModel(nn.Module):
    def __init__(self,numClasses=10,numCodeWords=16,lenCodeWord=12,numCodeBooks=12):
        super().__init__()
        self.Z=torch.empty(numCodeWords,numCodeBooks*lenCodeWord)
        self.Z=nn.init.xavier_normal_(self.Z)
        self.Z=Variable(self.Z,requires_grad=True)

        self.C=torch.empty(numClasses,numCodeBooks*lenCodeWord)
        self.C=nn.init.xavier_normal_(self.C)
        self.C=Variable(self.C,requires_grad=True)

        self.firstPartVGG=requiredLayers1
        self.secondPartVGG=requiredLayers2
        self.globalAvgPooling1=nn.AvgPool2d(56,stride=2)
        self.globalAvgPooling2=nn.AvgPool2d(56,stride=2)
        self.finalFeatureLayer=nn.Linear(768,lenCodeWord*numCodeBooks)

    def featureExtractor(self,input,finetune):
        #Extracted Features Portion
        for child in self.firstPartVGG.children():
            print(child)
            for param in child.parameters():
              param.requires_grad = finetune
        
        for child in self.secondPartVGG.children():
            print(child)
            for param in child.parameters():
              param.requires_grad = finetune

        intermediate1=self.firstPartVGG(input)
        pooledOutput1=self.globalAvgPooling1(intermediate1)
        intermediate2=self.secondPartVGG(intermediate1)
        pooledOutput2=self.globalAvgPooling2(intermediate2)
        concatenatedOutput=torch.cat((pooledOutput1,pooledOutput2),1)
        concatenatedOutput=torch.reshape(concatenatedOutput,(1,768))
        extractedFeatures=self.finalFeatureLayer(concatenatedOutput)
        return extractedFeatures

    def classifier(self,extractedFeatures,C):
        #Classifier portion
        x=torch.split(extractedFeatures,numCodeBooks,dim=1)
        y=torch.split(C,numCodeBooks,dim=1)
        for codeBookIndex in range(numCodeBooks):
            currentResult=torch.matmul(x[codeBookIndex], torch.transpose(y[codeBookIndex], 0, 1))
            if codeBookIndex==0:
                result=currentResult
            else:
                result=torch.cat((result,currentResult),0)
        logits=torch.mean(result,0)

        return logits

"""
input=torch.randn(1,3,224,224)
gpqModel=GPQModel(10,16,12,12)
extractedFeatures=gpqModel.featureExtractor(input,True)
logits=gpqModel.classifier(extractedFeatures,gpqModel.C)
print(extractedFeatures.shape)
print(logits.shape)
"""

def flipGradient(x,l=1.0):
    positivePath=torch.tensor(x*torch.tensor(l+1).type(torch.FloatTensor),requires_grad=False)
    negativePath=torch.tensor(-x*torch.tensor(l).type(torch.FloatTensor),requires_grad=True)
    return positivePath+negativePath

def IntraNorm(featureDescriptor,numCodeBooks):
    x=torch.split(featureDescriptor,numCodeBooks,dim=1)

    for codeBookIndex in range(numCodeBooks):
        l2NormValues=F.normalize(x[codeBookIndex],dim=1,p=2)
        #print("Shape : ",l2NormValues.shape)
        if codeBookIndex==0:
            inNorm=l2NormValues
        else:
            inNorm=torch.cat((inNorm,l2NormValues),1)
    
    return inNorm

def softAssignment(zValue, xValue, numCodeBooks,alpha):
    x=torch.split(xValue,numCodeBooks,dim=1)
    y=torch.split(zValue,numCodeBooks,dim=1)
    #print(len(x))
    #print(len(y))

    for codeBookIndex in range(numCodeBooks):
        sizeX=x[codeBookIndex].shape[0]
        sizeY=y[codeBookIndex].shape[0]
        #print("SizeX :",sizeX,"  SizeY : ",sizeY)

        firstDim,secondDim=x[codeBookIndex].shape
        #print("firstDim : ",firstDim,"  secondDim : ",secondDim)
        xx=torch.reshape(x[codeBookIndex].unsqueeze(0),(firstDim,secondDim,1))
        #print("Between xx : ",xx.shape)
        xx=xx.repeat(1,1,sizeY)
        #print("End xx : ",xx.shape)

        firstDim,secondDim=y[codeBookIndex].shape
        #print("firstDim : ",firstDim,"  secondDim : ",secondDim)
        yy=torch.reshape(y[codeBookIndex].unsqueeze(0),(firstDim,secondDim,1))
        #print("Between yy : ",yy.shape)
        yy=yy.repeat(1,1,sizeX).permute(2,1,0)
        #print("End yy : ",yy.shape)
        diff=1-torch.sum(torch.mul(xx,yy),dim=1)
        #print("Diff shape : ",diff.shape)
        softmaxFunction=nn.Softmax(dim=1)
        softmaxOutput=softmaxFunction(diff*(-alpha))
        #print("Softmax out shape : ",softmaxOutput.shape)
        multipliedValue=torch.matmul(softmaxOutput,y[codeBookIndex])
        #print(softDesTemp.shape)

        if codeBookIndex==0:
            featureDescriptor=multipliedValue
        else:
            featureDescriptor=torch.cat((featureDescriptor,multipliedValue),1)

    return IntraNorm(featureDescriptor,numCodeBooks)

"""
x=torch.randn(1,3,224,224)
z_=gpqModel.C
x_=gpqModel.Z
alpha=softAssgnAlpha
print(SoftAssignment(z_,x_,numCodeBooks,alpha).shape)
"""

def NPQLoss(labelsSimilarity,embeddingX,embeddingQ,numCodeBooks,regLambda=0.002):
    regAnchor=torch.mean(torch.sum(torch.square(embeddingX),dim=1))
    regPositive=torch.mean(torch.sum(torch.square(embeddingQ),dim=1))
    l2Loss=torch.mul(0.25*reg_lambda,regAnchor+regPositive)
    
    embeddingX=F.normalize(embeddingX,dim=1,p=2)
    embeddingQ=F.normalize(embeddingQ,dim=1,p=2)
    logits=torch.matmul(embeddingX,embeddingQ)*numCodeBooks

    lossValue=torch.sum(-labelsSimilarity * F.log_softmax(logits,-1),-1)
    meanLoss=lossValue.mean()

    return meanLoss+l2Loss
  
def CLSLoss(label,logits):
    lossValue=torch.sum(-label * F.log_softmax(logits,-1),-1)
    meanLoss=lossValue.mean()
    return meanLoss

def SMELoss(features,centroids,numSegments):
    x=torch.split(features,numSegments,dim=1)
    y=torch.split(centroids,numSegments,dim=0)

    for segmentIndex in range(numSegments):
        multipliedOutput=torch.matmul(x[i],y[i])
        currentLogits=torch.reshape(multipliedOutput.unsqueeze(0),(firstDim,secondDim,1))
        
        if segmentIndex==0:
            logits=currentLogits
        else:
            logits=torch.cat((logits,currentLogits),2)
    
    logits=F.softmax(torch.mean(logits,2),dim=1)
    lossValue=torch.sum(logits*(torch.log(logits+1e-5)),1)
    return torch.mean(lossValue)

"""
x=torch.randn(1,3,224,224)
Net=GPQModel(10,16,12,12)
alpha=20.0
beta = 4
extractedFeatures=gpqModel.featureExtractor(input,True)
logits=gpqModel.classifier(extractedFeatures,gpqModel.C)
print(extractedFeatures.shape)
print(logits.shape)

#Net = GPQModel(training=training_flag)
Prototypes = IntraNorm(Net.C, numCodeBooks)
Z = SoftAssignment(Prototypes, Net.Z, n_book, alpha)

feature_S = Net.featureExtractor(x,True)
feature_T = flipGradient(Net.featureExtractor(x_T))

feature_S = IntraNorm(feature_S, numCodeBooks)
feature_T = IntraNorm(feature_T, numCodeBooks)

descriptor_S = SoftAssignment(Z, feature_S, numCodeBooks, alpha)

logits_S = Net.classifier(feature_S * beta, tf.transpose(Prototypes) * beta)

hash_loss = N_PQ_loss(labels_Similarity=label_Mat, embeddings_x=feature_S, embeddings_q=descriptor_S, n_book)
cls_loss = CLS_loss(label, logits_S)
entropy_loss = SME_loss(feature_T * beta, tf.transpose(Prototypes) * beta, n_book)


import tensorflow as tf
image = tf.zeros([16,12])
image1=tf.zeros([10,12])
image2=tf.ones([10,12])
size_x=16
size_y=10
xx=tf.expand_dims(image,-1)
xx = tf.tile(xx, tf.stack([1, 1, size_y]))
print(xx.shape)
yy = tf.expand_dims(image1, -1)
yy = tf.tile(yy, tf.stack([1, 1, size_x]))
yy = tf.transpose(yy, perm=[2, 1, 0])
print(yy.shape)
mult=tf.multiply(xx,yy)
diff=1-tf.reduce_sum(tf.multiply(xx,yy),1)
diff=tf.nn.softmax(diff*(-softAssgnAlpha), axis=1)
print("Softmax : ",diff.shape)
print(image1.shape)
oft_des_tmp = tf.matmul(diff, image1, transpose_a=False, transpose_b=False)
print(oft_des_tmp.shape)
soft_des_tmp1 = tf.matmul(diff, image1, transpose_a=False, transpose_b=False)
soft_des_tmp2 = tf.matmul(diff, image2, transpose_a=False, transpose_b=False)
descriptor = tf.concat([soft_des_tmp1, soft_des_tmp2], axis=1)
print(descriptor.shape)

x=tf.split(descriptor,12, 1)
for i in range(12):
        norm_tmp = tf.nn.l2_normalize(x[i], axis=1)
        if i==0:
            innorm = norm_tmp
        else:
            innorm = tf.concat([innorm, norm_tmp], axis=1)
print(innorm.shape)
"""
