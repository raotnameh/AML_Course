{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import *\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "from data.data_loader import *\n",
    "from model import *\n",
    "import torch.optim as optim\n",
    "from  RetrievalTest import *\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "batchSize = 1\n",
    "\n",
    "print(\"Loading the dataset\")\n",
    "Source_x, Source_y, Target_x = prepare_Data(data_dir, True)\n",
    "Gallery_x, Query_x = prepare_Data(data_dir, False)\n",
    "similarity = csr_matrix(scipy.io.loadmat(\"data/cifar10/cifar10_Similarity.mat\")['label_Similarity']).todense()\n",
    "\n",
    "print(\"Data loading finished\")\n",
    "\n",
    "gallery = torch.utils.data.DataLoader(Gallery_x,batch_size=1000)\n",
    "query = torch.utils.data.DataLoader(Query_x,batch_size=1000)\n",
    "\n",
    "source = torch.utils.data.DataLoader([(Source_x[i], Source_y[i]) for i in range(len(Source_x))],batch_size=batchSize, shuffle=True)\n",
    "target = torch.utils.data.DataLoader(Target_x,batch_size=batchSize,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vggModel=models.vgg16_bn(pretrained=True)\n",
    "# vggModel.children\n",
    "\n",
    "net1, net2 = [], []\n",
    "for i in vggModel.children():\n",
    "    for r, i in enumerate(i.children()):\n",
    "        if r <=23: net1.append(i)\n",
    "        elif r<= 32: net2.append(i)\n",
    "    break\n",
    "net1, net2  = nn.Sequential(*net1), nn.Sequential(*net2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def intranorm(features,n_book):\n",
    "    x = features.split(n_book,1)\n",
    "    \n",
    "    for b in range(n_book):\n",
    "        if b==0: dummy = F.normalize(x[b],1)\n",
    "        else:\n",
    "            dummy = torch.cat((dummy,F.normalize(x[b],1)),1)\n",
    "    return dummy\n",
    "\n",
    "def shape_(inp):\n",
    "     for i in inp:\n",
    "            print(f\"shape is: {i.shape}\")\n",
    "            \n",
    "def Indexing_(Z,des,numSeg):\n",
    "        Z = intranorm(Z,numSeg)\n",
    "        x = torch.split(des,numSeg,1)\n",
    "        y = torch.split(Z,numSeg,1)\n",
    "        for i in range(numSeg):\n",
    "            size_x = x[i].shape[0]\n",
    "            size_y = y[i].shape[0]\n",
    "            xx = x[i].unsqueeze(-1)\n",
    "\n",
    "            dummy = torch.tensor(1)\n",
    "            xx = xx.tile([1,1,size_y])\n",
    "            yy = y[i].unsqueeze(-1)\n",
    "            yy = yy.tile([1,1,size_x])\n",
    "            yy = yy.permute(2,1,0)\n",
    "            diff = torch.sum(torch.multiply(xx,yy),1)\n",
    "\n",
    "            arg = torch.argmax(diff,1)\n",
    "            max_idx = arg.reshape(-1,1)\n",
    "\n",
    "            if i == 0: quant_idx = max_idx\n",
    "            else: quant_idx = torch.cat((quant_idx,max_idx),1)\n",
    "\n",
    "        return quant_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class softassignment_(nn.Module):\n",
    "    def __init__(self,len_code, n_book, intn_word):\n",
    "        super(softassignment_,self).__init__()\n",
    "        self.Z = nn.Linear(len_code * n_book,intn_word, bias=False)\n",
    "\n",
    "    def forward(self,features,n_book,alpha,):\n",
    "        z_ = intranorm(self.Z.state_dict()['weight'], n_book).split(n_book,1)\n",
    "        x_ = features.split(n_book,1)\n",
    "\n",
    "        for i in range(n_book):\n",
    "            size_z = z_[i].shape[0] # number of codewords\n",
    "            size_x = x_[i].shape[0] # batch size\n",
    "            xx = x_[i].unsqueeze(-1)\n",
    "            xx = xx.repeat(1,1,size_z)\n",
    "            zz = z_[i].unsqueeze(-1)\n",
    "            zz = zz.repeat(1,1,size_x).T\n",
    "\n",
    "            diff = 1 - torch.sum(torch.mul(xx,zz), 1) # 32,16\n",
    "            softmax_diff = F.softmax(diff*(-alpha),1) #32,16\n",
    "            soft_des_temp = torch.matmul(softmax_diff,z_[i]) # 32,12\n",
    "            if i == 0: descriptor = soft_des_temp\n",
    "            else: descriptor = torch.cat((descriptor,soft_des_temp),1)\n",
    "\n",
    "        return intranorm(descriptor,n_book) # 32,144"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class classifier_(nn.Module):\n",
    "    def __init__(self,n_CLASSES, len_code, n_book,):\n",
    "        super(classifier_,self).__init__()\n",
    "        self.prototypes = nn.Linear(len_code * n_book, n_CLASSES, bias=False)\n",
    "        self.n_book = n_book\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x_ = x.split(self.n_book,1)\n",
    "        c_ = (intranorm(self.prototypes.state_dict()['weight'], n_book)*beta).T.split(self.n_book,0)\n",
    "        for i in range(self.n_book):\n",
    "            sub_res = torch.matmul(x_[i], c_[i]).unsqueeze(-1)\n",
    "            if i == 0: res = sub_res\n",
    "            else: res = torch.cat((res,sub_res),2)\n",
    "        \n",
    "        return torch.sum(res, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class features_(nn.Module):\n",
    "    def __init__(self, net1, net2):\n",
    "        super(features_,self).__init__()\n",
    "        \n",
    "        self.net1 = net1\n",
    "        self.net2 = net2\n",
    "        self.gavgp = nn.AdaptiveAvgPool2d(1)\n",
    "        self.linear = nn.Linear(768, len_code*n_book)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = self.net1(x) # shape: torch.Size([32, 3, 32,32])>torch.Size([32, 3, 4, 4])\n",
    "        x_branch = self.gavgp(x)\n",
    "        x = self.net2(x) # shape: torch.Size([32, 3, 4, 4])> torch.Size([32, 3, 4, 4])\n",
    "        x = self.gavgp(x)\n",
    "        \n",
    "        x = torch.cat((x,x_branch),1)\n",
    "        \n",
    "        return self.linear(x.view(-1,768))\n",
    "    \n",
    "# x = torch.randn(32, 3, 32, 32, device='cpu')\n",
    "# model = features_(net1, net2)\n",
    "# out = model(x)# model\n",
    "# out.shape, out.split(12,1)[0].shape, model.Z.shape, model.prototypes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class flipGradient_(nn.Module):\n",
    "    def forward(self,x,l=1.0):\n",
    "        positivePath=(x*2).clone().detach().requires_grad_(False)\n",
    "        negativePath=(-x).requires_grad_(True)\n",
    "        return positivePath+negativePath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#models\n",
    "model = features_(net1, net2).to(device)\n",
    "classifier = classifier_(n_CLASSES, len_code, n_book).to(device)\n",
    "softassignment = softassignment_(len_code, n_book, intn_word).to(device)\n",
    "flipGradient = flipGradient_()\n",
    "\n",
    "# optimizer\n",
    "class_optim = optim.Adam(classifier.parameters(),lr=0.002,weight_decay=0.00001,amsgrad=True)\n",
    "model_optim = optim.Adam(model.parameters(),lr=0.0002,weight_decay=0.00001,amsgrad=True)\n",
    "soft_optim = optim.Adam(softassignment.parameters(),lr=0.002,weight_decay=0.00001,amsgrad=True)\n",
    "\n",
    "class_optim.zero_grad()\n",
    "model_optim.zero_grad()\n",
    "soft_optim.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils_ import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_path = '0_base_hemant.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if weights_path:\n",
    "    print(\"Loading weights\")\n",
    "    weights = torch.load(weights_path)\n",
    "    model.load_state_dict(weights['modelStateDict'][0])\n",
    "    classifier.load_state_dict(weights['modelStateDict'][1])\n",
    "    softassignment.load_state_dict(weights['modelStateDict'][2])\n",
    "\n",
    "    model_optim.load_state_dict(weights['modelOptimDict'][0])\n",
    "    class_optim.load_state_dict(weights['modelOptimDict'][1])\n",
    "    soft_optim(weights['modelOptimDict'][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, classifier, softassignment, gallery, query,Indexing_, pqDist, cat_apcal):\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        classifier.eval()\n",
    "        softassignment.eval()\n",
    "\n",
    "        for r, i in tqdm(enumerate(gallery)):\n",
    "            dfg = intranorm(model(i.to(device).permute(0,3,1,2)),n_book)\n",
    "            if r == 0: \n",
    "                dummy = dfg\n",
    "            else: \n",
    "                dummy = torch.cat((dummy, dfg), 0)\n",
    "\n",
    "        for r, i in tqdm(enumerate(query)):\n",
    "            dfg = intranorm(model(i.to(device).permute(0,3,1,2)),n_book)\n",
    "            if r == 0: \n",
    "                query_x = dfg\n",
    "            else: \n",
    "                query_x = torch.cat((query_x, dfg), 0)\n",
    "\n",
    "        dummy = Indexing_(softassignment.Z.state_dict()['weight'].cpu(), dummy.cpu(), n_book)\n",
    "        gallery_x = dummy.numpy().astype(int)\n",
    "        quantizedDist = pqDist(intranorm(softassignment.Z.state_dict()['weight'].cpu(), n_book), n_book,gallery_x, query_x.cpu().numpy()).T\n",
    "        Rank = np.argsort(quantizedDist, axis=0)\n",
    "        mean_average_precision=cat_apcal(similarity,Rank,54000)\n",
    "\n",
    "        return mean_average_precision\n",
    "\n",
    "save += f\"mAP_score: \\t{mean_average_precision}\"\n",
    "if mean_average_precision > score:\n",
    "    print(\"saving better vallidated model\")\n",
    "    score = mean_average_precision\n",
    "    stateToBeSaved={\n",
    "        'modelStateDict': [model.state_dict(),classifier.state_dict(), softassignment.state_dict()],\n",
    "        'modelOptimDict': [model_optim.state_dict(),class_optim.state_dict(), soft_optim.state_dict()],\n",
    "        'score': mean_average_precision,\n",
    "        'epoch': epoch+1}\n",
    "    torch.save(stateToBeSaved,f\"GPQ.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "target_ = iter(target)\n",
    "score = 0\n",
    "save = \"\"\n",
    "for epoch in tqdm(range()):\n",
    "    m_,n,o,p = 0,0,0,0\n",
    "    model.train()\n",
    "    classifier.train()\n",
    "    softassignment.train()\n",
    "    for df, batch in enumerate(source):\n",
    "        x, y = batch\n",
    "        x = torch.tensor(data_augmentation(x)).to(device)\n",
    "        try: \n",
    "            xu = next(target_)\n",
    "            xu = torch.tensor(data_augmentation(xu)).to(device)\n",
    "        except:\n",
    "            target_ = iter(target)\n",
    "            xu = next(target_)\n",
    "            xu = torch.tensor(data_augmentation(xu)).to(device)\n",
    "\n",
    "        features = intranorm(model(x.permute(0,3,1,2)), n_book)\n",
    "        featuresu = flipGradient(intranorm(model(xu.permute(0,3,1,2)), n_book))\n",
    "        quanta = softassignment(features,n_book,alpha)\n",
    "        logits = classifier(features *beta)\n",
    "\n",
    "        y = y.to(device)\n",
    "        cls_loss = torch.nn.functional.cross_entropy(logits,y)\n",
    "\n",
    "        y = torch.eye(n_CLASSES)[y].to(device)\n",
    "        entropy_loss = SMELoss(featuresu *beta ,intranorm(classifier.prototypes.state_dict()['weight'] *beta, n_book) , n_book)\n",
    "\n",
    "        y_ = torch.matmul(y,y.T)\n",
    "        y_ /= torch.sum(y_, axis=1, keepdims=True)\n",
    "        hash_loss = NPQLoss(y_,features, quanta,n_book)   \n",
    "        break\n",
    "        final_loss = hash_loss + cls_loss  + 0.01*entropy_loss \n",
    "        \n",
    "        o += cls_loss.item()\n",
    "        m_ += final_loss.item()\n",
    "        n += hash_loss.item()\n",
    "        p += entropy_loss.item()\n",
    "        \n",
    "        final_loss.backward()\n",
    "        \n",
    "        model_optim.step()\n",
    "        soft_optim.step()\n",
    "        class_optim.step()\n",
    "        \n",
    "        class_optim.zero_grad()\n",
    "        model_optim.zero_grad()\n",
    "        soft_optim.zero_grad()\n",
    "#     break\n",
    "    print(f\"epoch:{epoch+1}\\t{m_}\\t{n}\\t{o}\\t{p}\\n\")\n",
    "    save += f\"{m_/10}\\t{n/10}\\t{o/10}\\t{p/10}\\n\"\n",
    "    with open(\"ba_analysis.txt\", \"w\") as f:\n",
    "        f.write(save)\n",
    "    \n",
    "    if epoch % 5 == 0:\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            classifier.eval()\n",
    "            softassignment.eval()\n",
    "\n",
    "            for r, i in tqdm(enumerate(gallery)):\n",
    "                dfg = intranorm(model(i.to(device).permute(0,3,1,2)),n_book)\n",
    "                if r == 0: \n",
    "                    dummy = dfg\n",
    "                else: \n",
    "                    dummy = torch.cat((dummy, dfg), 0)\n",
    "\n",
    "            for r, i in tqdm(enumerate(query)):\n",
    "                dfg = intranorm(model(i.to(device).permute(0,3,1,2)),n_book)\n",
    "                if r == 0: \n",
    "                    query_x = dfg\n",
    "                else: \n",
    "                    query_x = torch.cat((query_x, dfg), 0)\n",
    "\n",
    "\n",
    "        dummy = Indexing_(softassignment.Z.state_dict()['weight'].cpu(), dummy.cpu(), n_book)\n",
    "        gallery_x = dummy.numpy().astype(int)\n",
    "        quantizedDist = pqDist(intranorm(softassignment.Z.state_dict()['weight'].cpu(), n_book), n_book,gallery_x, query_x.cpu().numpy()).T\n",
    "        Rank = np.argsort(quantizedDist, axis=0)\n",
    "        mean_average_precision=cat_apcal(similarity,Rank,54000)\n",
    "        save += f\"epoch+score: {df+1}/{epoch+1}\\t{final_loss.item()}\\t{hash_loss.item()}\\t{cls_loss.item()}\\t{entropy_loss.item()}\\t{mean_average_precision}\\n\"\n",
    "        if mean_average_precision > score:\n",
    "            score = mean_average_precision\n",
    "            stateToBeSaved={\n",
    "                'modelStateDict': [model.state_dict(),classifier.state_dict(), softassignment.state_dict()],\n",
    "                'score': mean_average_precision,}\n",
    "            torch.save(stateToBeSaved,f\"{epoch}_base_hemant.pth\")\n",
    "        print(mean_average_precision)\n",
    "    with open(\"ba_score.txt\", \"w\") as f:\n",
    "        f.write(save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if mean_average_precision > score:\n",
    "    score = mean_average_precision\n",
    "    stateToBeSaved={\n",
    "        'modelStateDict': [model.state_dict(),classifier.state_dict(), softassignment.state_dict()],\n",
    "        'score': mean_average_precision,}\n",
    "    torch.save(stateToBeSaved,f\"{epoch}_base_hemant.pth\")\n",
    "print(mean_average_precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_loss.shape, hash_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# THE END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------Loading dataset--------------------\n",
      "Source:  (5000, 32, 32, 3) (5000,)\n",
      "Target:  (54000, 32, 32, 3)\n",
      "--------------------Loading dataset--------------------\n",
      "Gallery:  (54000, 32, 32, 3) (54000,)\n",
      "Query:  (1000, 32, 32, 3) (1000,)\n"
     ]
    }
   ],
   "source": [
    "from config import *\n",
    "from data.data_loader import *\n",
    "from model import *\n",
    "from utils import *\n",
    "import torch.optim as optim\n",
    "    \n",
    "#Dataloader\n",
    "Source_x, Source_y, Target_x = prepare_Data(data_dir, True)\n",
    "source = torch.utils.data.DataLoader([(Source_x[i], Source_y[i]) for i in range(len(Source_x))],batch_size=batchSize, shuffle=True)\n",
    "target = torch.utils.data.DataLoader(Target_x,batch_size=batchSize,shuffle=True)\n",
    "\n",
    "Gallery_x, Query_x = prepare_Data(data_dir, False)\n",
    "gallery = torch.utils.data.DataLoader(Gallery_x,batch_size=2*batchSize)\n",
    "query = torch.utils.data.DataLoader(Query_x,batch_size=2*batchSize)\n",
    "similarity = csr_matrix(scipy.io.loadmat(\"data/cifar10/cifar10_Similarity.mat\")['label_Similarity']).todense()\n",
    "\n",
    "#models\n",
    "model = features_(net1, net2).to(device)\n",
    "classifier = classifier_(n_CLASSES, len_code, n_book).to(device)\n",
    "softassignment = softassignment_(len_code, n_book, intn_word).to(device)\n",
    "flipGradient = flipGradient_()\n",
    "\n",
    "# optimizer\n",
    "class_optim = optim.Adam(classifier.parameters(),lr=0.002,weight_decay=0.00001,amsgrad=True)\n",
    "model_optim = optim.Adam(model.parameters(),lr=0.0002,weight_decay=0.00001,amsgrad=True)\n",
    "soft_optim = optim.Adam(softassignment.parameters(),lr=0.002,weight_decay=0.00001,amsgrad=True)\n",
    "\n",
    "if weights_path:\n",
    "    print(\"Loading weights\")\n",
    "    weights = torch.load(weights_path)\n",
    "    \n",
    "    model.load_state_dict(weights['modelStateDict'][0])\n",
    "    classifier.load_state_dict(weights['modelStateDict'][1])\n",
    "    softassignment.load_state_dict(weights['modelStateDict'][2])\n",
    "\n",
    "    model_optim.load_state_dict(weights['modelOptimDict'][0])\n",
    "    class_optim.load_state_dict(weights['modelOptimDict'][1])\n",
    "    soft_optim(weights['modelOptimDict'][2])\n",
    "\n",
    "class_optim.zero_grad()\n",
    "model_optim.zero_grad()\n",
    "soft_optim.zero_grad()\n",
    "\n",
    "\n",
    "target_ = iter(target)\n",
    "score = 0\n",
    "save = \"\"\n",
    "for epoch in tqdm(range(total_epochs)):\n",
    "    m_,n,o,p = 0,0,0,0\n",
    "    model.train()\n",
    "    classifier.train()\n",
    "    softassignment.train()\n",
    "\n",
    "    for df, batch in enumerate(source):\n",
    "        x, y = batch\n",
    "        x = torch.tensor(data_augmentation(x)).to(device)\n",
    "        try: \n",
    "            xu = next(target_)\n",
    "            xu = torch.tensor(data_augmentation(xu)).to(device)\n",
    "        except:\n",
    "            target_ = iter(target)\n",
    "            xu = next(target_)\n",
    "            xu = torch.tensor(data_augmentation(xu)).to(device)\n",
    "\n",
    "        features = intranorm(model(x.permute(0,3,1,2)), n_book)\n",
    "        featuresu = flipGradient(intranorm(model(xu.permute(0,3,1,2)), n_book))\n",
    "        quanta = softassignment(features,n_book,alpha)\n",
    "        logits = classifier(features *beta)\n",
    "\n",
    "        y = y.to(device)\n",
    "        cls_loss = torch.nn.functional.cross_entropy(logits,y)\n",
    "\n",
    "        y = torch.eye(n_CLASSES)[y].to(device)\n",
    "        entropy_loss = SMELoss(featuresu *beta ,intranorm(classifier.prototypes.state_dict()['weight'] *beta, n_book) , n_book)\n",
    "\n",
    "        y_ = torch.matmul(y,y.T)\n",
    "        y_ /= torch.sum(y_, axis=1, keepdims=True)\n",
    "        hash_loss = NPQLoss(y_,features, quanta,n_book)   \n",
    "\n",
    "        final_loss = hash_loss + cls_loss  + 0.01*entropy_loss \n",
    "        break\n",
    "        o += cls_loss.item()\n",
    "        m_ += final_loss.item()\n",
    "        n += hash_loss.item()\n",
    "        p += entropy_loss.item()\n",
    "\n",
    "        final_loss.backward()\n",
    "\n",
    "        model_optim.step()\n",
    "        soft_optim.step()\n",
    "        class_optim.step()\n",
    "\n",
    "        class_optim.zero_grad()\n",
    "        model_optim.zero_grad()\n",
    "        soft_optim.zero_grad()\n",
    "\n",
    "    save += f\"Total_loss: {m_/10}\\t Hash_loss: {n/10}\\t Classsifier_loss: {o/10}\\t Entropy_loss: {p/10}\"\n",
    "\n",
    "    if epoch % test_term == 0: \n",
    "        mean_average_precision = test_(similarity, model, classifier, softassignment, gallery, query,Indexing_, pqDist, cat_apcal)\n",
    "        save += f\"mAP_score: \\t{mean_average_precision}\"\n",
    "        if mean_average_precision > score:\n",
    "            print(\"Found better validated model\")\n",
    "            score = mean_average_precision\n",
    "            stateToBeSaved={\n",
    "                'modelStateDict': [model.state_dict(),classifier.state_dict(), softassignment.state_dict()],\n",
    "                'modelOptimDict': [model_optim.state_dict(),class_optim.state_dict(), soft_optim.state_dict()],\n",
    "                'score': mean_average_precision,\n",
    "                'epoch': epoch+1}\n",
    "            torch.save(stateToBeSaved,f\"{model_save_path}/GPQ.pth\")\n",
    "    save += '\\n'\n",
    "    s_ = save.split('\\n')[-1]\n",
    "    print(f\"epoch:{epoch+1}\\t {s}\")\n",
    "    with open(\"loss.txt\", \"w\") as f:\n",
    "        f.write(save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b3dd93b4ea4497196a42640bc876be7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=300.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76a22727254849bc93e08fa87a663d66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17aa47195cb8429ea297f324ac767585",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found better validated model\n",
      "\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'models//GPQ.pth'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-3380469ccf0a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     64\u001b[0m                 \u001b[0;34m'score'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmean_average_precision\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m                 'epoch': epoch+1}\n\u001b[0;32m---> 66\u001b[0;31m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstateToBeSaved\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34mf\"{model_save_path}/GPQ.pth\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m     \u001b[0msave\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m'\\n'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0ms_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msave\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/deep/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization)\u001b[0m\n\u001b[1;32m    367\u001b[0m     \u001b[0m_check_dill_version\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpickle_module\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    368\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 369\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    370\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_use_new_zipfile_serialization\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    371\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0m_open_zipfile_writer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/deep/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'w'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/deep/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_open_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'models//GPQ.pth'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(stateToBeSaved,f\"{model_save_path}/GPQ.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
