{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the dataset\n",
      "--------------------Loading dataset--------------------\n",
      "Source:  (5000, 32, 32, 3) (5000,)\n",
      "Target:  (54000, 32, 32, 3)\n",
      "--------------------Loading dataset--------------------\n",
      "Gallery:  (54000, 32, 32, 3) (54000,)\n",
      "Query:  (1000, 32, 32, 3) (1000,)\n",
      "Data loading finished\n"
     ]
    }
   ],
   "source": [
    "from config import *\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "from data.data_loader import *\n",
    "from model import *\n",
    "import torch.optim as optim\n",
    "from  RetrievalTest import *\n",
    "\n",
    "batchSize = 4\n",
    "\n",
    "print(\"Loading the dataset\")\n",
    "Source_x, Source_y, Target_x = prepare_Data(data_dir, True)\n",
    "Gallery_x, Query_x = prepare_Data(data_dir, False)\n",
    "similarity = csr_matrix(scipy.io.loadmat(\"data/cifar10/cifar10_Similarity.mat\")['label_Similarity']).todense()\n",
    "\n",
    "print(\"Data loading finished\")\n",
    "\n",
    "gallery = torch.utils.data.DataLoader(Gallery_x,batch_size=1000)\n",
    "query = torch.utils.data.DataLoader(Query_x,batch_size=1000)\n",
    "\n",
    "source = torch.utils.data.DataLoader([(Source_x[i], Source_y[i]) for i in range(len(Source_x))],batch_size=batchSize, shuffle=True)\n",
    "target = torch.utils.data.DataLoader(Target_x,batch_size=batchSize,shuffle=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "vggModel=models.vgg16_bn(pretrained=True)\n",
    "# vggModel.children\n",
    "\n",
    "net1, net2 = [], []\n",
    "for i in vggModel.children():\n",
    "    for r, i in enumerate(i.children()):\n",
    "        if r <=23: net1.append(i)\n",
    "        elif r<= 32: net2.append(i)\n",
    "    break\n",
    "net1, net2  = nn.Sequential(*net1), nn.Sequential(*net2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def intranorm(features,n_book):\n",
    "    x = features.split(n_book,1)\n",
    "    \n",
    "    for b in range(n_book):\n",
    "        if b==0: dummy = F.normalize(x[b],1)\n",
    "        else:\n",
    "            dummy = torch.cat((dummy,F.normalize(x[b],1)),1)\n",
    "    return dummy\n",
    "\n",
    "def shape_(inp):\n",
    "     for i in inp:\n",
    "            print(f\"shape is: {i.shape}\")\n",
    "            \n",
    "def Indexing(Z,des,numSeg):\n",
    "        Z = intranorm(Z,numSeg)\n",
    "        x = torch.split(des,numSeg,1)\n",
    "        y = torch.split(Z,numSeg,1)\n",
    "        for i in range(numSeg):\n",
    "            size_x = x[i].shape[0]\n",
    "            size_y = y[i].shape[0]\n",
    "            xx = x[i].unsqueeze(-1)\n",
    "\n",
    "            dummy = torch.tensor(1)\n",
    "            xx = xx.tile([1,1,size_y])\n",
    "            yy = y[i].unsqueeze(-1)\n",
    "            yy = yy.tile([1,1,size_x])\n",
    "            yy = yy.permute(2,1,0)\n",
    "            diff = torch.sum(torch.multiply(xx,yy),1)\n",
    "\n",
    "            arg = torch.argmax(diff,1)\n",
    "            max_idx = arg.reshape(-1,1)\n",
    "\n",
    "            if i == 0: quant_idx = max_idx\n",
    "            else: quant_idx = torch.cat((quant_idx,max_idx),1)\n",
    "\n",
    "        return quant_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class softassignment_(nn.Module):\n",
    "    def __init__(self,len_code, n_book, intn_word):\n",
    "        super(softassignment_,self).__init__()\n",
    "        self.Z = nn.Linear(len_code * n_book,intn_word, bias=False)\n",
    "\n",
    "    def forward(self,features,n_book,alpha,):\n",
    "        z_ = intranorm(self.Z.state_dict()['weight'], n_book).split(n_book,1)\n",
    "        x_ = features.split(n_book,1)\n",
    "\n",
    "        for i in range(n_book):\n",
    "            size_z = z_[i].shape[0] # number of codewords\n",
    "            size_x = x_[i].shape[0] # batch size\n",
    "            xx = x_[i].unsqueeze(-1)\n",
    "            xx = xx.repeat(1,1,size_z)\n",
    "            zz = z_[i].unsqueeze(-1)\n",
    "            zz = zz.repeat(1,1,size_x).T\n",
    "\n",
    "            diff = 1 - torch.sum(torch.mul(xx,zz), 1) # 32,16\n",
    "            softmax_diff = F.softmax(diff*(-alpha),1) #32,16\n",
    "            soft_des_temp = torch.matmul(softmax_diff,z_[i]) # 32,12\n",
    "            if i == 0: descriptor = soft_des_temp\n",
    "            else: descriptor = torch.cat((descriptor,soft_des_temp),1)\n",
    "\n",
    "        return intranorm(descriptor,n_book) # 32,144"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class classifier_(nn.Module):\n",
    "    def __init__(self,n_CLASSES, len_code, n_book,):\n",
    "        super(classifier_,self).__init__()\n",
    "        self.prototypes = nn.Linear(len_code * n_book, n_CLASSES, bias=False)\n",
    "        self.n_book = n_book\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x_ = x.split(self.n_book,1)\n",
    "        c_ = (intranorm(self.prototypes.state_dict()['weight'], n_book)*beta).T.split(self.n_book,0)\n",
    "        for i in range(self.n_book):\n",
    "            sub_res = torch.matmul(x_[i], c_[i]).unsqueeze(-1)\n",
    "            if i == 0: res = sub_res\n",
    "            else: res = torch.cat((res,sub_res),2)\n",
    "        \n",
    "        return torch.sum(res, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class features_(nn.Module):\n",
    "    def __init__(self, net1, net2):\n",
    "        super(features_,self).__init__()\n",
    "        \n",
    "        self.net1 = net1\n",
    "        self.net2 = net2\n",
    "        self.gavgp = nn.AdaptiveAvgPool2d(1)\n",
    "        self.linear = nn.Linear(768, len_code*n_book)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = self.net1(x) # shape: torch.Size([32, 3, 32,32])>torch.Size([32, 3, 4, 4])\n",
    "        x_branch = self.gavgp(x)\n",
    "        x = self.net2(x) # shape: torch.Size([32, 3, 4, 4])> torch.Size([32, 3, 4, 4])\n",
    "        x = self.gavgp(x)\n",
    "        \n",
    "        x = torch.cat((x,x_branch),1)\n",
    "        \n",
    "        return self.linear(x.view(-1,768))\n",
    "    \n",
    "# x = torch.randn(32, 3, 32, 32, device='cpu')\n",
    "# model = features_(net1, net2)\n",
    "# out = model(x)# model\n",
    "# out.shape, out.split(12,1)[0].shape, model.Z.shape, model.prototypes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class flipGradient_(nn.Module):\n",
    "    def forward(self,x,l=1.0):\n",
    "        positivePath=(x*2).clone().detach().requires_grad_(False)\n",
    "        negativePath=(-x).requires_grad_(True)\n",
    "        return positivePath+negativePath\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#models\n",
    "model = features_(net1, net2).to(device)\n",
    "classifier = classifier_(n_CLASSES, len_code, n_book).to(device)\n",
    "softassignment = softassignment_(len_code, n_book, intn_word).to(device)\n",
    "flipGradient = flipGradient_()\n",
    "# optimizer\n",
    "class_optim = optim.Adam(classifier.parameters(),lr=0.0002,betas=(0.5,0.999))\n",
    "model_optim = optim.Adam(model.parameters(),lr=0.0002,betas=(0.5,0.999))\n",
    "soft_optim = optim.Adam(softassignment.parameters(),lr=0.0002,betas=(0.5,0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7b414acb9994e7c9c02af1bc1c34c1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.6359, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "target_ = iter(target)\n",
    "score = 0\n",
    "save = \"\"\n",
    "for epoch in tqdm(range(1)):\n",
    "    m = 0\n",
    "    model.train()\n",
    "    classifier.train()\n",
    "    softassignment.train()\n",
    "    for df, batch in enumerate(source):\n",
    "        x, y = batch\n",
    "        x = torch.tensor(data_augmentation(x)).to(device)\n",
    "        try: \n",
    "            xu = next(target_)\n",
    "            xu = torch.tensor(data_augmentation(xu)).to(device)\n",
    "        except:\n",
    "            target_ = iter(target)\n",
    "            xu = next(target_)\n",
    "            xu = torch.tensor(data_augmentation(xu)).to(device)\n",
    "\n",
    "        features = intranorm(model(x.permute(0,3,1,2)), n_book)\n",
    "        featuresu = flipGradient(intranorm(model(xu.permute(0,3,1,2)), n_book))\n",
    "        quanta = softassignment(features,n_book,alpha)\n",
    "        logits = classifier(features* beta)\n",
    "        \n",
    "        y = y.to(device)\n",
    "        \n",
    "        cls_loss = torch.nn.functional.cross_entropy(logits,y)\n",
    "        print(torch.nn.functional.cross_entropy(logits,y))\n",
    "#         break\n",
    "        y = torch.eye(numClasses)[y].to(device)\n",
    "        entropy_loss = SMELoss(featuresu*beta ,intranorm(classifier.prototypes.state_dict()['weight'], n_book)*beta , n_book)\n",
    "#         break\n",
    "        y_ = torch.matmul(y,y.T)\n",
    "#         y_ /= torch.sum(y_, axis=1, keepdims=True)\n",
    "        break\n",
    "        hash_loss = NPQLoss(y_,features, quanta,n_book)   \n",
    "        \n",
    "        final_loss = hash_loss + 0.1*entropy_loss + 0.1*cls_loss \n",
    "        m += final_loss.item()\n",
    "        \n",
    "        class_optim.zero_grad()\n",
    "        model_optim.zero_grad()\n",
    "        soft_optim.zero_grad()\n",
    "        \n",
    "        final_loss.backward()\n",
    "        \n",
    "        class_optim.step()\n",
    "        model_optim.step()\n",
    "        soft_optim.step()\n",
    "        \n",
    "        save += f\"epoch: {df+1}/{epoch+1}\\t{final_loss.item()}\\t{hash_loss.item()}\\t{cls_loss.item()}\\t{entropy_loss.item()}\\n\"\n",
    "        print(f\"epoch: {df+1}/{epoch+1}\\t{final_loss.item()}\\t{hash_loss.item()}\\t{cls_loss.item()}\\t{entropy_loss.item()}\\n\")\n",
    "       \n",
    "    \n",
    "    if False: #epoch % 50 == 0:\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            classifier.eval()\n",
    "            softassignment.eval()\n",
    "\n",
    "            for r, i in tqdm(enumerate(gallery)):\n",
    "                if r == 0: \n",
    "                    dummy = model(i.to(device).permute(0,3,1,2))\n",
    "                else: \n",
    "                    dummy = torch.cat((dummy, model(i.to(device).permute(0,3,1,2))), 0)\n",
    "\n",
    "            for r, i in tqdm(enumerate(query)):\n",
    "                if r == 0: \n",
    "                    query_x = model(i.to(device).permute(0,3,1,2))\n",
    "                else: \n",
    "                    query_x = torch.cat((query_x, model(i.to(device).permute(0,3,1,2))), 0)\n",
    "\n",
    "\n",
    "        dummy = Indexing(intranorm(softassignment.Z.state_dict()['weight'].cpu(), n_book), dummy.cpu(), n_book)\n",
    "        gallery_x = dummy.numpy().astype(int)\n",
    "        quantizedDist = pqDist(intranorm(softassignment.Z.state_dict()['weight'].cpu(), n_book), n_book,gallery_x, query_x.cpu().numpy()).T\n",
    "        Rank = np.argsort(quantizedDist, axis=0)\n",
    "        mean_average_precision=cat_apcal(similarity,Rank,54000)\n",
    "        save += f\"epoch+score: {df+1}/{epoch+1}\\t{final_loss.item()}\\t{hash_loss.item()}\\t{cls_loss.item()}\\t{entropy_loss.item()}\\t{mean_average_precision}\\n\"\n",
    "        if mean_average_precision > score:\n",
    "            score = mean_average_precision\n",
    "            stateToBeSaved={\n",
    "                'modelStateDict': [model.state_dict(),classifier.state_dict(), softassignment.state_dict()],\n",
    "                'score': mean_average_precision,}\n",
    "            torch.save(stateToBeSaved,f\"{epoch}_copy_hemant.pth\")\n",
    "        print(mean_average_precision)\n",
    "    with open(\"analysis_copy.txt\", \"w\") as f:\n",
    "        f.write(save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 0, 0, 1],\n",
       "        [0, 1, 1, 0],\n",
       "        [0, 1, 1, 0],\n",
       "        [1, 0, 0, 1]], device='cuda:0')"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_.long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "regAnchor=torch.mean(torch.sum(torch.square(features),dim=1))\n",
    "regPositive=torch.mean(torch.sum(torch.square(quanta),dim=1))\n",
    "l2Loss=torch.mul(1,regAnchor+regPositive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.0767, device='cuda:0', grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l2Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.0064, -2.0064, -2.0064, -1.0064]], device='cuda:0')"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.log_softmax(y_[0].view(1,-1),1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., -inf, -inf, 0.],\n",
       "        [-inf, 0., 0., -inf],\n",
       "        [-inf, 0., 0., -inf],\n",
       "        [0., -inf, -inf, 0.]], device='cuda:0')"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.log(y_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (4) must match the size of tensor b (10) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-61-bc5ac1ae119a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0my_\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (4) must match the size of tensor b (10) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "y_ * torch.log(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[0.1, 0.2, 0.7] (prediction) ------------------ [1.0, 0.0, 0.0] (target)\n",
    "\n",
    "what you want is - (1.0 * log(0.1) + 0.0 * log(0.2) + 0.0 * log(0.7)) this is the cross entropy lossaa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "multi-target not supported at /opt/conda/conda-bld/pytorch_1608538128634/work/aten/src/THCUNN/generic/ClassNLLCriterion.cu:15",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-58-6086a8bee9f7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.conda/envs/deep/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[1;32m   2437\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2438\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2439\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2440\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/deep/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mnll_loss\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[1;32m   2233\u001b[0m                          .format(input.size(0), target.size(0)))\n\u001b[1;32m   2234\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2235\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2236\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2237\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnll_loss2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: multi-target not supported at /opt/conda/conda-bld/pytorch_1608538128634/work/aten/src/THCUNN/generic/ClassNLLCriterion.cu:15"
     ]
    }
   ],
   "source": [
    "torch.nn.functional.cross_entropy(y_,y_.long())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "        model.eval()\n",
    "        classifier.eval()\n",
    "        softassignment.eval()\n",
    "\n",
    "        for r, i in tqdm(enumerate(gallery)):\n",
    "            if r == 0: \n",
    "                dummy = model(i.to(device).permute(0,3,1,2))\n",
    "            else: \n",
    "                dummy = torch.cat((dummy, model(i.to(device).permute(0,3,1,2))), 0)\n",
    "\n",
    "        for r, i in tqdm(enumerate(query)):\n",
    "            if r == 0: \n",
    "                query_x = model(i.to(device).permute(0,3,1,2))\n",
    "            else: \n",
    "                query_x = torch.cat((query_x, model(i.to(device).permute(0,3,1,2))), 0)\n",
    "\n",
    "\n",
    "dummy = Indexing(intranorm(softassignment.Z.state_dict()['weight'].cpu(), n_book), dummy.cpu(), n_book)\n",
    "gallery_x = dummy.numpy().astype(int)\n",
    "quantizedDist = pqDist(intranorm(softassignment.Z.state_dict()['weight'].cpu(), n_book), n_book,gallery_x, query_x.cpu().numpy()).T\n",
    "Rank = np.argsort(quantizedDist, axis=0)\n",
    "mean_average_precision = cat_apcal(similarity,Rank,54000)\n",
    "mean_average_precision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# THE END"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
